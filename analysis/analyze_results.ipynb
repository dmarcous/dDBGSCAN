{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.cluster import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_key = \"baseline\"\n",
    "ours_key = \"ours\"\n",
    "\n",
    "default_dataset = \"geonames\"\n",
    "\n",
    "baseline_variant_name = 'mrdbscan'\n",
    "ours_variant_name = 'dDBGSCAN'\n",
    "default_variants = {baseline_key: baseline_variant_name, ours_key: ours_variant_name}\n",
    "partitioner_only_variants = {baseline_key: ours_variant_name, ours_key: ours_variant_name}\n",
    "\n",
    "baseline_partitioner_name = 'cost'\n",
    "ours_partitioner_name = 'S2'\n",
    "default_partitioners = {baseline_key: baseline_partitioner_name, ours_key: ours_partitioner_name}\n",
    "\n",
    "default_exp_index=1\n",
    "default_exp_indices = {baseline_key: default_exp_index, ours_key: default_exp_index}\n",
    "dummy_partition_level=99\n",
    "default_partition_level=13\n",
    "default_partition_levels = {baseline_key: default_partition_level, ours_key: default_partition_level}\n",
    "default_1part_points=100000000\n",
    "default_dummy_max_points=256\n",
    "default_max_points = {baseline_key: default_1part_points, ours_key: default_dummy_max_points}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_output_base_path=\"s3://waze.tmp/dmarcous/clustering/output/\"\n",
    "local_output_base_path=\"/tmp/output/\"\n",
    "# local_output_base_path=\"~/git/dDBGSCAN/src/test/resources/output/\" # Test \n",
    "experiment_path_suffix=\"{dataset}/{variant}/{partitioner}/partlvl_{partition_level}/maxp_{maxpoints}/exp_{exp_index:02d}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildExperimentSpec(dataset, variant, partitioner, partition_level, maxpoints, exp_index):\n",
    "    return {\"dataset\": dataset, \"variant\": variant,\n",
    "            \"partitioner\": partitioner, \"partition_level\": partition_level,\n",
    "            \"maxpoints\": maxpoints, \"exp_index\": exp_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatExperimentPath(base_path, experiment_spec):\n",
    "    results_template = base_path + experiment_path_suffix\n",
    "    result_path=results_template.format(dataset=experiment_spec['dataset'],\n",
    "                                        variant=experiment_spec['variant'],\n",
    "                                        partitioner=experiment_spec['partitioner'],\n",
    "                                        partition_level=experiment_spec['partition_level'],\n",
    "                                        maxpoints=experiment_spec['maxpoints'],\n",
    "                                        exp_index=experiment_spec['exp_index'])\n",
    "    return result_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadResultFiles(experiment_spec):\n",
    "    remote_path = formatExperimentPath(remote_output_base_path, experiment_spec)\n",
    "    local_path = formatExperimentPath(local_output_base_path, experiment_spec)\n",
    "    download_cmd = \"aws s3 cp --recursive \" + remote_path + \" \" + local_path\n",
    "    print(download_cmd)\n",
    "    if not os.path.isdir(local_path):\n",
    "        return_code = os.system(download_cmd)\n",
    "    else :\n",
    "        print(\"Skipping, local already exists\")\n",
    "    return local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readClusteringResults(data_path):\n",
    "    column_names = ['recordId', 'cluster', 'instanceStatus']\n",
    "    df = pd.concat([pd.read_csv(f, index_col=None, names=column_names) for f in glob.glob(data_path+'*')], axis=0, ignore_index=True, sort=False)\n",
    "    return df.sort_values(by=['recordId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataForAnaysis(dataset=default_dataset, variant=ours_variant_name,\n",
    "                          partitioner=ours_partitioner_name,\n",
    "                          partition_level=default_partition_level, maxpoints=default_1part_points,\n",
    "                          exp_index=default_exp_index):\n",
    "    experiment_spec=buildExperimentSpec(dataset, variant, partitioner, partition_level, maxpoints, exp_index)\n",
    "    print(experiment_spec)\n",
    "    local_path = downloadResultFiles(experiment_spec)\n",
    "    results_df = readClusteringResults(local_path)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resultsToLabels(df):\n",
    "    return df.sort_values(by=['recordId'])['cluster'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareClusteringResults(baseline, ours):\n",
    "    assert baseline.shape[0] == ours.shape[0]\n",
    "    return adjusted_rand_score(resultsToLabels(baseline), resultsToLabels(ours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustClusterIds(df):\n",
    "    df_adjusted = df.copy()\n",
    "    for cluster in df['cluster'].unique():\n",
    "        df_adjusted.loc[df_adjusted.cluster==cluster,'cluster'] = df_adjusted.loc[df_adjusted.cluster==cluster,'recordId'].min()\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(dataset,\n",
    "            variants=partitioner_only_variants,\n",
    "            partitioners=default_partitioners,\n",
    "            partition_levels=default_partition_levels,\n",
    "            max_points=default_max_points,\n",
    "            exp_indices=default_exp_indices):\n",
    "    baseline = prepareDataForAnaysis(dataset,\n",
    "                                     variants[baseline_key], partitioners[baseline_key],\n",
    "                                     partition_levels[baseline_key], max_points[baseline_key],\n",
    "                                     exp_indices[baseline_key])\n",
    "    ours = prepareDataForAnaysis(dataset,\n",
    "                                 variants[ours_key], partitioners[ours_key],\n",
    "                                 partition_levels[ours_key], max_points[ours_key],\n",
    "                                 exp_indices[ours_key])\n",
    "    return compareClusteringResults(baseline, ours)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCompareResults(dataset, ARI):\n",
    "    print(\"Dateset : \" + dataset + \" , ARI: \" + str(ARI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_datasets=[\"geo54k\", \"geo108k\", \"geo005\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale ours VS cost based partitioner version of ours - scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo54k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geo54k , ARI: 1.0\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/ /tmp/output/geo108k/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geo108k , ARI: 0.9999987538300177\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo005/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/ /tmp/output/geo005/dDBGSCAN/cost/partlvl_13/maxp_100000000/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo005/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo005/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geo005 , ARI: 1.0\n"
     ]
    }
   ],
   "source": [
    "for dataset in small_datasets:\n",
    "    ARI = compare(dataset)\n",
    "    printCompareResults(dataset, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo015/dDBGSCAN/cost/partlvl_13/maxp_1659300/exp_01/ /tmp/output/geo015/dDBGSCAN/cost/partlvl_13/maxp_1659300/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo015/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo015/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geo015 , ARI: 0.9999999391659478\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geoquarter/dDBGSCAN/cost/partlvl_13/maxp_2765500/exp_01/ /tmp/output/geoquarter/dDBGSCAN/cost/partlvl_13/maxp_2765500/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geoquarter/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geoquarter/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geoquarter , ARI: 0.9999990655379201\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geosmall/dDBGSCAN/cost/partlvl_13/maxp_5531000/exp_01/ /tmp/output/geosmall/dDBGSCAN/cost/partlvl_13/maxp_5531000/exp_01/\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geosmall/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geosmall/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Dateset : geosmall , ARI: 0.9999992241253113\n"
     ]
    }
   ],
   "source": [
    "large_datasets = [\"geo015\", \"geoquarter\", \"geosmall\"]\n",
    "large_max_points = [1659300, 2765500, 5531000]\n",
    "\n",
    "for dataset, maxp in zip(large_datasets, large_max_points):\n",
    "    cur_max_points = {baseline_key: maxp, ours_key: default_dummy_max_points}\n",
    "    ARI = compare(dataset, max_points=cur_max_points)\n",
    "    printCompareResults(dataset, ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours - different partition levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"geonames\"\n",
    "ours_partitioner = {baseline_key: ours_partitioner_name, ours_key: ours_partitioner_name}\n",
    "ours_max_points = {baseline_key: default_dummy_max_points, ours_key: default_dummy_max_points}\n",
    "part_levels = [9] #11, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'geonames', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 7, 'maxpoints': 256, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geonames/dDBGSCAN/S2/partlvl_7/maxp_256/exp_01/ /tmp/output/geonames/dDBGSCAN/S2/partlvl_7/maxp_256/exp_01/\n",
      "{'dataset': 'geonames', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 11, 'maxpoints': 256, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geonames/dDBGSCAN/S2/partlvl_11/maxp_256/exp_01/ /tmp/output/geonames/dDBGSCAN/S2/partlvl_11/maxp_256/exp_01/\n",
      "Dateset : geonames , ARI: 0.9999991362803113\n"
     ]
    }
   ],
   "source": [
    "for lvl in part_levels:\n",
    "    cur_partition_levels = {baseline_key: 7, ours_key: lvl}\n",
    "    ARI = compare(dataset, partitioners=ours_partitioner, partition_levels=cur_partition_levels, max_points=ours_max_points)\n",
    "    printCompareResults(dataset, ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours - different runs of same experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"geo108k\"\n",
    "ours_partitioner = {baseline_key: ours_partitioner_name, ours_key: ours_partitioner_name}\n",
    "ours_max_points = {baseline_key: default_dummy_max_points, ours_key: default_dummy_max_points}\n",
    "experiments = [2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'geo108k', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 13, 'maxpoints': 256, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "{'dataset': 'geo108k', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 13, 'maxpoints': 256, 'exp_index': 2}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_02/ /tmp/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_02/\n",
      "Dateset : geo108k , ARI: 1.0\n",
      "{'dataset': 'geo108k', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 13, 'maxpoints': 256, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/ /tmp/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_01/\n",
      "Skipping, local already exists\n",
      "{'dataset': 'geo108k', 'variant': 'dDBGSCAN', 'partitioner': 'S2', 'partition_level': 13, 'maxpoints': 256, 'exp_index': 3}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_03/ /tmp/output/geo108k/dDBGSCAN/S2/partlvl_13/maxp_256/exp_03/\n",
      "Dateset : geo108k , ARI: 1.0\n"
     ]
    }
   ],
   "source": [
    "for exp in experiments:\n",
    "    cur_exps = {baseline_key: default_exp_index, ours_key: exp}\n",
    "    ARI = compare(dataset, partitioners=ours_partitioner, max_points=ours_max_points, exp_indices=cur_exps)\n",
    "    printCompareResults(dataset, ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours - cost partitioner - different maxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"geo54k\"\n",
    "ours_partitioner = {baseline_key: baseline_partitioner_name, ours_key: baseline_partitioner_name}\n",
    "maxps = [27000, 13500, 6750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 54000, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/\n",
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 27000, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_27000/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_27000/exp_01/\n",
      "Dateset : geo54k , ARI: 1.0\n",
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 54000, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/\n",
      "Skipping, local already exists\n",
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 13500, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_13500/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_13500/exp_01/\n",
      "Dateset : geo54k , ARI: 1.0\n",
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 54000, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_54000/exp_01/\n",
      "Skipping, local already exists\n",
      "{'dataset': 'geo54k', 'variant': 'dDBGSCAN', 'partitioner': 'cost', 'partition_level': 13, 'maxpoints': 6750, 'exp_index': 1}\n",
      "aws s3 cp --recursive s3://waze.tmp/dmarcous/clustering/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_6750/exp_01/ /tmp/output/geo54k/dDBGSCAN/cost/partlvl_13/maxp_6750/exp_01/\n",
      "Dateset : geo54k , ARI: 1.0\n"
     ]
    }
   ],
   "source": [
    "for maxp in maxps:\n",
    "    ours_max_points = {baseline_key: 54000, ours_key: maxp}\n",
    "    ARI = compare(dataset, partitioners=ours_partitioner, max_points=ours_max_points)\n",
    "    printCompareResults(dataset, ARI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
